{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best practices for One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding is a common technique used to convert categorical data into a numerical format. However, it's essential to know when it's appropriate to apply it, as it's primarily designed for categorical features, not target variables in certain models like logistic regression. Here’s a guide on when and when not to use one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use One-Hot Encoding:\n",
    "### 1. For Categorical Input Features:\n",
    "\n",
    "##### Context: \n",
    "If your dataset has categorical features (e.g., gender, country, product type) that are non-numerical and need to be converted into a numerical form.\n",
    "##### Why: \n",
    "Algorithms like logistic regression, decision trees, and support vector machines require numerical inputs. One-hot encoding transforms each category into a binary column (0 or 1).\n",
    "##### Example:\n",
    "- Original feature: Color = [\"Red\", \"Green\", \"Blue\"]\n",
    "- After one-hot encoding:\n",
    "\n",
    "|Color_Red | Color_Green | Color_Blue|\n",
    "|----------|-------------|-----------|\n",
    "|1         |0            |0|\n",
    "|0         |1            |0|\n",
    "|0         |0            |1|\n",
    "\n",
    "##### Models where it applies: \n",
    "- Logistic regression, \n",
    "- decision trees, \n",
    "- random forest, \n",
    "- SVM, \n",
    "- k-nearest neighbors (KNN), \n",
    "- neural networks.\n",
    "\n",
    "### 2. When Input Features are Categorical and There’s No Ordinal Relationship:\n",
    "\n",
    "##### Context: \n",
    "Use one-hot encoding when the categorical variable does not have an inherent order. For example, Color or Country doesn’t have a natural ranking.\n",
    "##### Why:\n",
    "One-hot encoding ensures that the model doesn’t impose an unintended ordinal relationship (i.e., one category being larger or smaller than another).\n",
    "\n",
    "### 3. Neural Networks (For Inputs and Outputs in Certain Cases):\n",
    "\n",
    "##### Context (inputs):\n",
    "One-hot encoding is often used for input features in neural networks, especially when feeding categorical data into the network.\n",
    "##### Context (outputs): \n",
    "For multi-class classification tasks in neural networks, you typically one-hot encode the target variable (output) because the final layer of the network often uses a softmax function to output probabilities for each class.\n",
    "##### Why: \n",
    "Neural networks work well with numerical inputs, and one-hot encoded outputs make sense for classification tasks using softmax activation in the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When Not to Use One-Hot Encoding:\n",
    "### 1. For Target Variables in Most Classification Models (like Logistic Regression):\n",
    "\n",
    "##### Context: \n",
    "Logistic regression and similar classifiers (like decision trees, SVM) expect the target variable (y) to be a 1D array of class labels, not a one-hot encoded matrix.\n",
    "##### Why:\n",
    "These algorithms predict the probability or class label directly, without needing one-hot encoding for the target variable.\n",
    "##### Exception: \n",
    "If you're working with neural networks for multi-class classification, the target (y) is typically one-hot encoded because the network outputs a probability distribution for each class.\n",
    "\n",
    "### 2. For Ordinal Categorical Features:\n",
    "\n",
    "##### Context: \n",
    "If your categorical feature has an ordinal relationship (e.g., Education Level = [\"High School\", \"Bachelors\", \"Masters\", \"PhD\"]), you should not use one-hot encoding.\n",
    "##### Why: \n",
    "In this case, there’s a clear order (PhD is higher than Bachelors), and one-hot encoding would discard this information. Instead, use label encoding to retain the ordinal relationship.\n",
    "##### What to Use Instead: \n",
    "Use integer encoding or label encoding for ordinal features, where categories are mapped to integers preserving their order.\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Education_Level = [\"High School\", \"Bachelors\", \"Masters\", \"PhD\"]\n",
    "Label_Encoding: [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. When There are Too Many Categories:\n",
    "\n",
    "##### Context: \n",
    "One-hot encoding can result in a very sparse matrix if the categorical feature has many unique categories (e.g., thousands of categories in a City or Product ID feature).\n",
    "##### Why: \n",
    "This can lead to high dimensionality, increasing computation time and memory usage, and possibly leading to overfitting.\n",
    "##### What to Use Instead: \n",
    "You can use target encoding or embedding techniques (especially for neural networks) to reduce dimensionality.\n",
    "- Target Encoding: \n",
    "Replace the categories with the mean target value for each category.\n",
    "- Embeddings: Used in deep learning to represent categorical variables in a dense, lower-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary Table\n",
    "|Use One-Hot Encoding|Avoid One-Hot Encoding|\n",
    "|--------------------|----------------------|\n",
    "|Categorical input features with no ordinal relationship|Ordinal categorical features (use label encoding)|\n",
    "|Categorical variables as inputs for most ML models (e.g., logistic regression, decision trees, SVM)|Target variables for most models like logistic regression, SVM, decision trees|\n",
    "|Multi-class outputs for neural networks (for softmax activation)|When features have too many unique categories (use embeddings or target encoding)|\n",
    "|When converting categorical input features into binary columns|Target variable in non-neural network models|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding (For Input Features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Price    0    1    2    3\n",
      "0     10  0.0  1.0  0.0  1.0\n",
      "1     15  1.0  0.0  1.0  0.0\n",
      "2     20  0.0  0.0  0.0  0.0\n",
      "3     10  0.0  1.0  0.0  1.0\n",
      "4     25  0.0  0.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Example dataset\n",
    "data = pd.DataFrame({\n",
    "    'Color': ['Red', 'Green', 'Blue', 'Red', 'Blue'],\n",
    "    'Size': ['S', 'M', 'L', 'S', 'L'],\n",
    "    'Price': [10, 15, 20, 10, 25]\n",
    "})\n",
    "\n",
    "# One-hot encode the categorical features (Color, Size)\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')  # Drop first to avoid multicollinearity\n",
    "encoded_features = encoder.fit_transform(data[['Color', 'Size']])\n",
    "\n",
    "# Combine encoded features with the original dataset\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out())\n",
    "final_df = pd.concat([data['Price'], encoded_df], axis=1)\n",
    "\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "Use One-Hot Encoding when dealing with categorical input features, especially for non-ordinal data. Most machine learning algorithms (like logistic regression, decision trees, etc.) require numerical inputs, so one-hot encoding is perfect for converting categorical variables.\n",
    "Don’t use One-Hot Encoding for target variables in models like logistic regression. Instead, keep your target as a 1D array of labels unless you're working with neural networks and using softmax for classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QAGeneral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
